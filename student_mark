data1=[(1,"Steve"),(2,"David"),(3,"John"),(4,"Shree"),(5,"Helen")]
data2=[(1,"SQL",90),(1,"PySpark",100),(2,"SQL",70),(2,"PySpark",60),(3,"SQL",30),(3,"PySpark",20),(4,"SQL",50),(4,"PySpark",50),(5,"SQL",45),(5,"PySpark",45)]

schema1=["Id","Name"]
schema2=["Id","Subject","Mark"]

df1=spark.createDataFrame(data1,schema1)
df2=spark.createDataFrame(data2,schema2)
display(df1)
display(df2)
df_join=df1.join(df2,df1.Id==df2.Id).drop(df2.Id)
display(df_join)

from pyspark.sql.functions import *
df_per=df_join.groupBy('Id','Name').agg(
    (sum('Mark')/count('*')).alias('Percentage')
)
display(df_per)


df_final=df_per.select('*',
          when(df.percentage>=75, 'Distionction')
          .when((df.percentage<70) && (df.percentage>=60), 'first class')
          .when((df.percentage<60) && (df.percentage>=50), 'Second class')
          .when((df.percentage<50) && (df.percentage>=40), 'Third class')
          .when(df.percentage<=40, 'Fail')

)

display(df_final)








)
